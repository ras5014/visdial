{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import argparse\n",
    "import itertools\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from bisect import bisect\n",
    "\n",
    "# All File Imports\n",
    "import import_ipynb\n",
    "from dataset import VisDialDataset\n",
    "from encoder import LateFusionEncoder\n",
    "from decoder import DiscriminativeDecoder\n",
    "from utils.metrics import SparseGTMetrics, NDCG\n",
    "from model import EncoderDecoderModel\n",
    "# checkpointing file missing (Resolve later)\n",
    "from utils.checkpointing import CheckpointManager, load_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--load-pthpath'], dest='load_pthpath', nargs=None, const=None, default='', type=None, choices=None, help='To continue training, path to .pth file of saved checkpoint.', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--config-yml\",\n",
    "    default=\"configs/lf_disc_faster_rcnn_x101.yml\",\n",
    "    help=\"Path to a config file listing reader, model and solver parameters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train-json\",\n",
    "    default=\"data/visdial_1.0_train.json\",\n",
    "    help=\"Path to json file containing VisDial v1.0 training data.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--val-json\",\n",
    "    default=\"data/visdial_1.0_val.json\",\n",
    "    help=\"Path to json file containing VisDial v1.0 validation data.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--val-dense-json\",\n",
    "    default=\"data/visdial_1.0_val_dense_annotations.json\",\n",
    "    help=\"Path to json file containing VisDial v1.0 validation dense ground \"\n",
    "    \"truth annotations.\",\n",
    ")\n",
    "\n",
    "\n",
    "parser.add_argument_group(\n",
    "    \"Arguments independent of experiment reproducibility\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gpu-ids\",\n",
    "    nargs=\"+\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"List of ids of GPUs to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cpu-workers\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Number of CPU workers for dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overfit\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Overfit model on 5 examples, meant for debugging.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--validate\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to validate on val split after every epoch.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--in-memory\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Load the whole dataset and pre-extracted image features in memory. \"\n",
    "    \"Use only in presence of large RAM, atleast few tens of GBs.\",\n",
    ")\n",
    "\n",
    "\n",
    "parser.add_argument_group(\"Checkpointing related arguments\")\n",
    "parser.add_argument(\n",
    "    \"--save-dirpath\",\n",
    "    default=\"checkpoints/\",\n",
    "    help=\"Path of directory to create checkpoint directory and save \"\n",
    "    \"checkpoints.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load-pthpath\",\n",
    "    default=\"\",\n",
    "    help=\"To continue training, path to .pth file of saved checkpoint.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the trainning Configuration\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abc/anaconda3/envs/visdial3/lib/python3.6/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# args code leaving\n",
    "# Will write later if required\n",
    "args = parser.parse_args([])\n",
    "config = yaml.load(open(args.config_yml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  concat_history: true\n",
      "  image_features_test_h5: data/features_faster_rcnn_x101_test.h5\n",
      "  image_features_train_h5: data/features_faster_rcnn_x101_train.h5\n",
      "  image_features_val_h5: data/features_faster_rcnn_x101_val.h5\n",
      "  img_norm: 1\n",
      "  max_sequence_length: 20\n",
      "  vocab_min_count: 5\n",
      "  word_counts_json: data/visdial_1.0_word_counts_train.json\n",
      "model:\n",
      "  decoder: disc\n",
      "  dropout: 0.5\n",
      "  encoder: lf\n",
      "  img_feature_size: 2048\n",
      "  lstm_hidden_size: 512\n",
      "  lstm_num_layers: 2\n",
      "  word_embedding_size: 300\n",
      "solver:\n",
      "  batch_size: 128\n",
      "  initial_lr: 0.01\n",
      "  lr_gamma: 0.1\n",
      "  lr_milestones:\n",
      "  - 4\n",
      "  - 7\n",
      "  - 10\n",
      "  num_epochs: 20\n",
      "  training_splits: train\n",
      "  warmup_epochs: 1\n",
      "  warmup_factor: 0.2\n",
      "\n",
      "config_yml          : configs/lf_disc_faster_rcnn_x101.yml\n",
      "train_json          : data/visdial_1.0_train.json\n",
      "val_json            : data/visdial_1.0_val.json\n",
      "val_dense_json      : data/visdial_1.0_val_dense_annotations.json\n",
      "gpu_ids             : [0]\n",
      "cpu_workers         : 4\n",
      "overfit             : False\n",
      "validate            : False\n",
      "in_memory           : False\n",
      "save_dirpath        : checkpoints/\n",
      "load_pthpath        : \n"
     ]
    }
   ],
   "source": [
    "# Adding device code\n",
    "if isinstance(args.gpu_ids, int):\n",
    "    args.gpu_ids = [args.gpu_ids]\n",
    "device = (\n",
    "    torch.device(\"cuda\", args.gpu_ids[0])\n",
    "    if args.gpu_ids[0] >= 0\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# Print config and args.\n",
    "print(yaml.dump(config, default_flow_style=False))\n",
    "for arg in vars(args):\n",
    "    print(\"{:<20}: {}\".format(arg, getattr(args, arg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETTING UP DATASET, DATALOADER, MODEL, CRITERION, OPTIMIZER, SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/376083 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376083/376083 [00:26<00:00, 14408.26it/s]\n",
      "  0%|          | 847/337528 [00:00<00:40, 8320.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337528/337528 [00:20<00:00, 16489.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123287/123287 [00:08<00:00, 15041.58it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VisDialDataset(\n",
    "    config[\"dataset\"],\n",
    "    args.train_json,\n",
    "    overfit=args.overfit,\n",
    "    in_memory=args.in_memory,\n",
    "    num_workers=args.cpu_workers,\n",
    "    # Below two lines are added for disc decoder\n",
    "    return_options=True if config[\"model\"][\"decoder\"] == \"disc\" else False,\n",
    "    add_boundary_toks=False if config[\"model\"][\"decoder\"] == \"disc\" else True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"solver\"][\"batch_size\"],\n",
    "    num_workers=args.cpu_workers,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1724/45238 [00:00<00:02, 17227.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45238/45238 [00:02<00:00, 20000.75it/s]\n",
      "  5%|▍         | 1634/34822 [00:00<00:02, 16330.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34822/34822 [00:01<00:00, 19314.28it/s]\n",
      "100%|██████████| 2064/2064 [00:00<00:00, 18206.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "val_dataset = VisDialDataset(\n",
    "    config[\"dataset\"],\n",
    "    args.val_json,\n",
    "    args.val_dense_json,\n",
    "    overfit=args.overfit,\n",
    "    in_memory=args.in_memory,\n",
    "    num_workers=args.cpu_workers,\n",
    "    return_options=True,\n",
    "    # Below line is added for disc decoder\n",
    "    add_boundary_toks=False if config[\"model\"][\"decoder\"] == \"disc\" else True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"solver\"][\"batch_size\"]\n",
    "    # Below two lines ll be used, only used for disc decoder\n",
    "    if config[\"model\"][\"decoder\"] == \"disc\"\n",
    "    else 5,\n",
    "    num_workers=args.cpu_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: lf\n",
      "Decoder: disc\n"
     ]
    }
   ],
   "source": [
    "# Passing vocabulary to construct Embedding layer\n",
    "encoder = LateFusionEncoder(config[\"model\"], train_dataset.vocabulary)\n",
    "decoder = DiscriminativeDecoder(config[\"model\"], train_dataset.vocabulary)\n",
    "\n",
    "print(\"Encoder: {}\".format(config[\"model\"][\"encoder\"]))\n",
    "print(\"Decoder: {}\".format(config[\"model\"][\"decoder\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share word embedding b/w encoder & decoder\n",
    "decoder.word_embed = encoder.word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping encoder & decoder model in model to train\n",
    "model = EncoderDecoderModel(encoder, decoder).to(device)\n",
    "if -1 not in args.gpu_ids:\n",
    "    model = nn.DataParallel(model, args.gpu_ids)\n",
    "\n",
    "# Loss function\n",
    "if config[\"model\"][\"decoder\"] == \"disc\":\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "elif config[\"model\"][\"decoder\"] == \"gen\":\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        ignore_index=train_dataset.vocabulary.PAD_INDEX\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Trainning iteration Calculation\n",
    "if config[\"solver\"][\"training_splits\"] == \"trainval\":\n",
    "    iterations = (len(train_dataset) + len(val_dataset)) // config[\"solver\"][\"batch_size\"] + 1\n",
    "else:\n",
    "    iterations = len(train_dataset) // config[\"solver\"][\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the learning rate multiplier based on the current iteration\n",
    "# We are doing this because here the learning rate is not constant for the entire duration of trainning\n",
    "\n",
    "\"\"\"\n",
    "Returns a learning rate multiplier.\n",
    "Till `warmup_epochs`, learning rate linearly increases to `initial_lr`,\n",
    "and then gets multiplied by `lr_gamma` every time a milestone is crossed.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Optimizer: It initializes an Adamax optimizer for the model's parameters, using the initial learning rate specified in the configuration.\n",
    "Scheduler: It sets up a LambdaLR scheduler, which allows specifying a custom learning rate lambda function (lr_lambda) for updating the learning rate based on the optimizer and the defined lr_lambda_fun.\n",
    "\"\"\"\n",
    "\n",
    "def lr_lambda_fun(current_iteration: int) -> float:\n",
    "    current_epoch = float(current_iteration) / iterations\n",
    "    if current_epoch <= config[\"solver\"][\"warmup_epochs\"]:\n",
    "        alpha = current_epoch / float(config[\"solver\"][\"warmup_epochs\"])\n",
    "        return config[\"solver\"][\"warmup_factor\"] * (1.0 - alpha) + alpha\n",
    "    else:\n",
    "        idx = bisect(config[\"solver\"][\"lr_milestones\"], current_epoch)\n",
    "        return pow(config[\"solver\"][\"lr_gamma\"], idx)\n",
    "\n",
    "optimizer = optim.Adamax(model.parameters(), lr=config[\"solver\"][\"initial_lr\"])\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Before Trainning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = SummaryWriter(log_dir=args.save_dirpath)\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    model, optimizer, args.save_dirpath, config=config\n",
    ")\n",
    "sparse_metrics = SparseGTMetrics()\n",
    "ndcg = NDCG()\n",
    "\n",
    "# Below some code is written to start trainning from the checkpoint path\n",
    "if args.load_pthpath == \"\":\n",
    "    start_epoch = 0\n",
    "else:\n",
    "    # Here you can put path to checkpoint\n",
    "    start_epoch = int(args.load_pthpath.split(\"_\")[-1][:-4])\n",
    "\n",
    "    model_state_dict, optimizer_state_dict = load_checkpoint(args.load_pthpath)\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(model_state_dict)\n",
    "    else:\n",
    "        model.load_state_dict(model_state_dict)\n",
    "    optimizer.load_state_dict(optimizer_state_dict)\n",
    "    print(\"Loaded model from {}\".format(args.load_pthpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:15, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "view() received an invalid combination of arguments - got (dict, int, int, int), but expected one of:\n * (tuple of ints size)\n * (torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1fa504a4a5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Forward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Here we are getting the target for the specific train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         target = (\n",
      "\u001b[0;32m~/anaconda3/envs/visdial3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/visdial3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/visdial3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VisDial3/visdial/model.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/visdial3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VisDial3/visdial/encoder.ipynb\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: view() received an invalid combination of arguments - got (dict, int, int, int), but expected one of:\n * (tuple of ints size)\n * (torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "# To keep track of iterations (for tensorboard log)\n",
    "global_iteration_step = start_epoch * iterations\n",
    "\n",
    "for epoch in range(start_epoch, config[\"solver\"][\"num_epochs\"]):\n",
    "\n",
    "    # At the starting of epoch, Combine dataloaders if training on train + val data\n",
    "    if config[\"solver\"][\"training_splits\"] == \"trainval\":\n",
    "        combined_dataloader = itertools.chain(train_dataloader, val_dataloader)\n",
    "    else:\n",
    "        combined_dataloader = itertools.chain(train_dataloader)\n",
    "    \n",
    "    print(f\"\\nTraining for epoch {epoch}:\")\n",
    "    for i, batch in enumerate(tqdm(combined_dataloader)):\n",
    "        # Transfer to Device\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(device)\n",
    "\n",
    "        # Zeroing out gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        output = model(batch)\n",
    "        # Here we are getting the target for the specific train data\n",
    "        target = (\n",
    "            batch[\"ans_ind\"]\n",
    "            if config[\"model\"][\"decoder\"] == \"disc\"\n",
    "            else batch[\"ans_out\"]\n",
    "        )\n",
    "        # Loss computation \n",
    "        batch_loss = criterion(\n",
    "            output.view(-1, output.size(-1)), target.view(-1)\n",
    "        )\n",
    "\n",
    "        # Backward pass & optimization\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Below code is for tensorboard's Summary Writer\n",
    "        summary_writer.add_scalar(\n",
    "            \"train/loss\", batch_loss, global_iteration_step\n",
    "        )\n",
    "        summary_writer.add_scalar(\n",
    "            \"train/lr\", optimizer.param_groups[0][\"lr\"], global_iteration_step\n",
    "        )\n",
    "\n",
    "        scheduler.step(global_iteration_step)\n",
    "        global_iteration_step += 1\n",
    "    \n",
    "    # This we use to free up cache memory in GPU\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
