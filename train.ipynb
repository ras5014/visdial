{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from decoder.ipynb\n",
      "importing Jupyter notebook from /home/abc/Desktop/VisDial3/visdial/utils/metrics.ipynb\n",
      "importing Jupyter notebook from model.ipynb\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import argparse\n",
    "#import itertools\n",
    "#from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from bisect import bisect\n",
    "\n",
    "# All File Imports\n",
    "import import_ipynb\n",
    "from dataset import VisDialDataset\n",
    "from encoder import LateFusionEncoder\n",
    "from decoder import DiscriminativeDecoder\n",
    "from utils.metrics import SparseGTMetrics, NDCG\n",
    "from model import EncoderDecoderModel\n",
    "# checkpointing file missing (Resolve later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--load-pthpath'], dest='load_pthpath', nargs=None, const=None, default='', type=None, choices=None, help='To continue training, path to .pth file of saved checkpoint.', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--config-yml\",\n",
    "    default=\"configs/lf_disc_faster_rcnn_x101.yml\",\n",
    "    help=\"Path to a config file listing reader, model and solver parameters.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train-json\",\n",
    "    default=\"data/visdial_1.0_train.json\",\n",
    "    help=\"Path to json file containing VisDial v1.0 training data.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--val-json\",\n",
    "    default=\"data/visdial_1.0_val.json\",\n",
    "    help=\"Path to json file containing VisDial v1.0 validation data.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--val-dense-json\",\n",
    "    default=\"data/visdial_1.0_val_dense_annotations.json\",\n",
    "    help=\"Path to json file containing VisDial v1.0 validation dense ground \"\n",
    "    \"truth annotations.\",\n",
    ")\n",
    "\n",
    "\n",
    "parser.add_argument_group(\n",
    "    \"Arguments independent of experiment reproducibility\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gpu-ids\",\n",
    "    nargs=\"+\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"List of ids of GPUs to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cpu-workers\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Number of CPU workers for dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overfit\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Overfit model on 5 examples, meant for debugging.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--validate\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to validate on val split after every epoch.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--in-memory\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Load the whole dataset and pre-extracted image features in memory. \"\n",
    "    \"Use only in presence of large RAM, atleast few tens of GBs.\",\n",
    ")\n",
    "\n",
    "\n",
    "parser.add_argument_group(\"Checkpointing related arguments\")\n",
    "parser.add_argument(\n",
    "    \"--save-dirpath\",\n",
    "    default=\"checkpoints/\",\n",
    "    help=\"Path of directory to create checkpoint directory and save \"\n",
    "    \"checkpoints.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load-pthpath\",\n",
    "    default=\"\",\n",
    "    help=\"To continue training, path to .pth file of saved checkpoint.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the trainning Configuration\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abc/anaconda3/envs/visdial3/lib/python3.6/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# args code leaving\n",
    "# Will write later if required\n",
    "args = parser.parse_args([])\n",
    "config = yaml.load(open(args.config_yml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  concat_history: true\n",
      "  image_features_test_h5: data/features_faster_rcnn_x101_test.h5\n",
      "  image_features_train_h5: data/features_faster_rcnn_x101_train.h5\n",
      "  image_features_val_h5: data/features_faster_rcnn_x101_val.h5\n",
      "  img_norm: 1\n",
      "  max_sequence_length: 20\n",
      "  vocab_min_count: 5\n",
      "  word_counts_json: data/visdial_1.0_word_counts_train.json\n",
      "model:\n",
      "  decoder: disc\n",
      "  dropout: 0.5\n",
      "  encoder: lf\n",
      "  img_feature_size: 2048\n",
      "  lstm_hidden_size: 512\n",
      "  lstm_num_layers: 2\n",
      "  word_embedding_size: 300\n",
      "solver:\n",
      "  batch_size: 128\n",
      "  initial_lr: 0.01\n",
      "  lr_gamma: 0.1\n",
      "  lr_milestones:\n",
      "  - 4\n",
      "  - 7\n",
      "  - 10\n",
      "  num_epochs: 20\n",
      "  training_splits: train\n",
      "  warmup_epochs: 1\n",
      "  warmup_factor: 0.2\n",
      "\n",
      "config_yml          : configs/lf_disc_faster_rcnn_x101.yml\n",
      "train_json          : data/visdial_1.0_train.json\n",
      "val_json            : data/visdial_1.0_val.json\n",
      "val_dense_json      : data/visdial_1.0_val_dense_annotations.json\n",
      "gpu_ids             : [0]\n",
      "cpu_workers         : 4\n",
      "overfit             : False\n",
      "validate            : False\n",
      "in_memory           : False\n",
      "save_dirpath        : checkpoints/\n",
      "load_pthpath        : \n"
     ]
    }
   ],
   "source": [
    "# Adding device code\n",
    "if isinstance(args.gpu_ids, int):\n",
    "    args.gpu_ids = [args.gpu_ids]\n",
    "device = (\n",
    "    torch.device(\"cuda\", args.gpu_ids[0])\n",
    "    if args.gpu_ids[0] >= 0\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "# Print config and args.\n",
    "print(yaml.dump(config, default_flow_style=False))\n",
    "for arg in vars(args):\n",
    "    print(\"{:<20}: {}\".format(arg, getattr(args, arg)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETTING UP DATASET, DATALOADER, MODEL, CRITERION, OPTIMIZER, SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/376083 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 376083/376083 [00:26<00:00, 14408.26it/s]\n",
      "  0%|          | 847/337528 [00:00<00:40, 8320.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337528/337528 [00:20<00:00, 16489.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] Tokenizing captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123287/123287 [00:08<00:00, 15041.58it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VisDialDataset(\n",
    "    config[\"dataset\"],\n",
    "    args.train_json,\n",
    "    overfit=args.overfit,\n",
    "    in_memory=args.in_memory,\n",
    "    num_workers=args.cpu_workers,\n",
    "    # Below two lines are added for disc decoder\n",
    "    return_options=True if config[\"model\"][\"decoder\"] == \"disc\" else False,\n",
    "    add_boundary_toks=False if config[\"model\"][\"decoder\"] == \"disc\" else True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"solver\"][\"batch_size\"],\n",
    "    num_workers=args.cpu_workers,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1724/45238 [00:00<00:02, 17227.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45238/45238 [00:02<00:00, 20000.75it/s]\n",
      "  5%|▍         | 1634/34822 [00:00<00:02, 16330.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34822/34822 [00:01<00:00, 19314.28it/s]\n",
      "100%|██████████| 2064/2064 [00:00<00:00, 18206.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val2018] Tokenizing captions...\n"
     ]
    }
   ],
   "source": [
    "val_dataset = VisDialDataset(\n",
    "    config[\"dataset\"],\n",
    "    args.val_json,\n",
    "    args.val_dense_json,\n",
    "    overfit=args.overfit,\n",
    "    in_memory=args.in_memory,\n",
    "    num_workers=args.cpu_workers,\n",
    "    return_options=True,\n",
    "    # Below line is added for disc decoder\n",
    "    add_boundary_toks=False if config[\"model\"][\"decoder\"] == \"disc\" else True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"solver\"][\"batch_size\"]\n",
    "    # Below two lines ll be used, only used for disc decoder\n",
    "    if config[\"model\"][\"decoder\"] == \"disc\"\n",
    "    else 5,\n",
    "    num_workers=args.cpu_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: lf\n",
      "Decoder: disc\n"
     ]
    }
   ],
   "source": [
    "# Passing vocabulary to construct Embedding layer\n",
    "encoder = LateFusionEncoder(config[\"model\"], train_dataset.vocabulary)\n",
    "decoder = DiscriminativeDecoder(config[\"model\"], train_dataset.vocabulary)\n",
    "\n",
    "print(\"Encoder: {}\".format(config[\"model\"][\"encoder\"]))\n",
    "print(\"Decoder: {}\".format(config[\"model\"][\"decoder\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share word embedding b/w encoder & decoder\n",
    "decoder.word_embed = encoder.word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping encoder & decoder model in model to train\n",
    "model = EncoderDecoderModel(encoder, decoder).to(device)\n",
    "if -1 not in args.gpu_ids:\n",
    "    model = nn.DataParallel(model, args.gpu_ids)\n",
    "\n",
    "# Loss function\n",
    "if[\"model\"][\"decoder\"] == \"disc\":\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "elif config[\"model\"][\"decoder\"] == \"gen\":\n",
    "    criterion = nn.CrossEntropyLoss(\n",
    "        ignore_index=train_dataset.vocabulary.PAD_INDEX\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if config[\"solver\"][\"training_splits\"] == \"trainval\":\n",
    "    iterations = (len(train_dataset) + )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
