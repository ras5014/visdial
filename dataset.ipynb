{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from readers.ipynb\n",
      "importing Jupyter notebook from vocabulary.ipynb\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import import_ipynb\n",
    "from readers import (\n",
    "    DialogsReader,\n",
    "    DenseAnnotationsReader,\n",
    "    ImageFeaturesHdfReader,\n",
    ")\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "class VisDialDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A full representation of VisDial v1.0 (train/val/test) dataset. According\n",
    "    to the appropriate split, it returns dictionary of question, image,\n",
    "    history, ground truth answer, answer options, dense annotations etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Dict[str, Any],\n",
    "        dialogs_jsonpath: str,\n",
    "        dense_annotations_jsonpath: Optional[str] = None,\n",
    "        overfit: bool = False,\n",
    "        in_memory: bool = False,\n",
    "        num_workers: int = 1,\n",
    "        return_options: bool = True,\n",
    "        add_boundary_toks: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.return_options = return_options\n",
    "        self.add_boundary_toks = add_boundary_toks\n",
    "        self.dialogs_reader = DialogsReader(\n",
    "            dialogs_jsonpath,\n",
    "            num_examples=(5 if overfit else None),\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        if \"val\" in self.split and dense_annotations_jsonpath is not None:\n",
    "            self.annotations_reader = DenseAnnotationsReader(\n",
    "                dense_annotations_jsonpath\n",
    "            )\n",
    "        else:\n",
    "            self.annotations_reader = None\n",
    "\n",
    "        self.vocabulary = Vocabulary(\n",
    "            config[\"word_counts_json\"], min_count=config[\"vocab_min_count\"]\n",
    "        )\n",
    "\n",
    "        # Initialize image features reader according to split.\n",
    "        image_features_hdfpath = config[\"image_features_train_h5\"]\n",
    "        if \"val\" in self.dialogs_reader.split:\n",
    "            image_features_hdfpath = config[\"image_features_val_h5\"]\n",
    "        elif \"test\" in self.dialogs_reader.split:\n",
    "            image_features_hdfpath = config[\"image_features_test_h5\"]\n",
    "\n",
    "        self.hdf_reader = ImageFeaturesHdfReader(\n",
    "            image_features_hdfpath, in_memory\n",
    "        )\n",
    "\n",
    "        # Keep a list of image_ids as primary keys to access data.\n",
    "        self.image_ids = list(self.dialogs_reader.dialogs.keys())\n",
    "        if overfit:\n",
    "            self.image_ids = self.image_ids[:5]\n",
    "\n",
    "    @property\n",
    "    def split(self):\n",
    "        return self.dialogs_reader.split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image_id, which serves as a primary key for current instance.\n",
    "        image_id = self.image_ids[index]\n",
    "\n",
    "        # Get image features for this image_id using hdf reader.\n",
    "        image_features = self.hdf_reader[image_id]\n",
    "        image_features = torch.tensor(image_features)\n",
    "        # Normalize image features at zero-th dimension (since there's no batch\n",
    "        # dimension).\n",
    "        if self.config[\"img_norm\"]:\n",
    "            image_features = normalize(image_features, dim=0, p=2)\n",
    "\n",
    "        # Retrieve instance for this image_id using json reader.\n",
    "        visdial_instance = self.dialogs_reader[image_id]\n",
    "        caption = visdial_instance[\"caption\"]\n",
    "        dialog = visdial_instance[\"dialog\"]\n",
    "\n",
    "        # Convert word tokens of caption, question, answer and answer options\n",
    "        # to integers.\n",
    "        caption = self.vocabulary.to_indices(caption)\n",
    "        for i in range(len(dialog)):\n",
    "            dialog[i][\"question\"] = self.vocabulary.to_indices(\n",
    "                dialog[i][\"question\"]\n",
    "            )\n",
    "            if self.add_boundary_toks:\n",
    "                dialog[i][\"answer\"] = self.vocabulary.to_indices(\n",
    "                    [self.vocabulary.SOS_TOKEN]\n",
    "                    + dialog[i][\"answer\"]\n",
    "                    + [self.vocabulary.EOS_TOKEN]\n",
    "                )\n",
    "            else:\n",
    "                dialog[i][\"answer\"] = self.vocabulary.to_indices(\n",
    "                    dialog[i][\"answer\"]\n",
    "                )\n",
    "\n",
    "            if self.return_options:\n",
    "                for j in range(len(dialog[i][\"answer_options\"])):\n",
    "                    if self.add_boundary_toks:\n",
    "                        dialog[i][\"answer_options\"][\n",
    "                            j\n",
    "                        ] = self.vocabulary.to_indices(\n",
    "                            [self.vocabulary.SOS_TOKEN]\n",
    "                            + dialog[i][\"answer_options\"][j]\n",
    "                            + [self.vocabulary.EOS_TOKEN]\n",
    "                        )\n",
    "                    else:\n",
    "                        dialog[i][\"answer_options\"][\n",
    "                            j\n",
    "                        ] = self.vocabulary.to_indices(\n",
    "                            dialog[i][\"answer_options\"][j]\n",
    "                        )\n",
    "\n",
    "        questions, question_lengths = self._pad_sequences(\n",
    "            [dialog_round[\"question\"] for dialog_round in dialog]\n",
    "        )\n",
    "        history, history_lengths = self._get_history(\n",
    "            caption,\n",
    "            [dialog_round[\"question\"] for dialog_round in dialog],\n",
    "            [dialog_round[\"answer\"] for dialog_round in dialog],\n",
    "        )\n",
    "        answers_in, answer_lengths = self._pad_sequences(\n",
    "            [dialog_round[\"answer\"][:-1] for dialog_round in dialog]\n",
    "        )\n",
    "        answers_out, _ = self._pad_sequences(\n",
    "            [dialog_round[\"answer\"][1:] for dialog_round in dialog]\n",
    "        )\n",
    "\n",
    "        # Collect everything as tensors for ``collate_fn`` of dataloader to\n",
    "        # work seamlessly questions, history, etc. are converted to\n",
    "        # LongTensors, for nn.Embedding input.\n",
    "        item = {}\n",
    "        item[\"img_ids\"] = torch.tensor(image_id).long()\n",
    "        item[\"img_feat\"] = image_features\n",
    "        item[\"ques\"] = questions.long()\n",
    "        item[\"hist\"] = history.long()\n",
    "        item[\"ans_in\"] = answers_in.long()\n",
    "        item[\"ans_out\"] = answers_out.long()\n",
    "        item[\"ques_len\"] = torch.tensor(question_lengths).long()\n",
    "        item[\"hist_len\"] = torch.tensor(history_lengths).long()\n",
    "        item[\"ans_len\"] = torch.tensor(answer_lengths).long()\n",
    "        item[\"num_rounds\"] = torch.tensor(\n",
    "            visdial_instance[\"num_rounds\"]\n",
    "        ).long()\n",
    "\n",
    "        if self.return_options:\n",
    "            if self.add_boundary_toks:\n",
    "                answer_options_in, answer_options_out = [], []\n",
    "                answer_option_lengths = []\n",
    "                for dialog_round in dialog:\n",
    "                    options, option_lengths = self._pad_sequences(\n",
    "                        [\n",
    "                            option[:-1]\n",
    "                            for option in dialog_round[\"answer_options\"]\n",
    "                        ]\n",
    "                    )\n",
    "                    answer_options_in.append(options)\n",
    "\n",
    "                    options, _ = self._pad_sequences(\n",
    "                        [\n",
    "                            option[1:]\n",
    "                            for option in dialog_round[\"answer_options\"]\n",
    "                        ]\n",
    "                    )\n",
    "                    answer_options_out.append(options)\n",
    "\n",
    "                    answer_option_lengths.append(option_lengths)\n",
    "                answer_options_in = torch.stack(answer_options_in, 0)\n",
    "                answer_options_out = torch.stack(answer_options_out, 0)\n",
    "\n",
    "                item[\"opt_in\"] = answer_options_in.long()\n",
    "                item[\"opt_out\"] = answer_options_out.long()\n",
    "                item[\"opt_len\"] = torch.tensor(answer_option_lengths).long()\n",
    "            else:\n",
    "                answer_options = []\n",
    "                answer_option_lengths = []\n",
    "                for dialog_round in dialog:\n",
    "                    options, option_lengths = self._pad_sequences(\n",
    "                        dialog_round[\"answer_options\"]\n",
    "                    )\n",
    "                    answer_options.append(options)\n",
    "                    answer_option_lengths.append(option_lengths)\n",
    "                answer_options = torch.stack(answer_options, 0)\n",
    "\n",
    "                item[\"opt\"] = answer_options.long()\n",
    "                item[\"opt_len\"] = torch.tensor(answer_option_lengths).long()\n",
    "\n",
    "            if \"test\" not in self.split:\n",
    "                answer_indices = [\n",
    "                    dialog_round[\"gt_index\"] for dialog_round in dialog\n",
    "                ]\n",
    "                item[\"ans_ind\"] = torch.tensor(answer_indices).long()\n",
    "\n",
    "        # Gather dense annotations.\n",
    "        if \"val\" in self.split:\n",
    "            dense_annotations = self.annotations_reader[image_id]\n",
    "            item[\"gt_relevance\"] = torch.tensor(\n",
    "                dense_annotations[\"gt_relevance\"]\n",
    "            ).float()\n",
    "            item[\"round_id\"] = torch.tensor(\n",
    "                dense_annotations[\"round_id\"]\n",
    "            ).long()\n",
    "\n",
    "        return item\n",
    "\n",
    "    def _pad_sequences(self, sequences: List[List[int]]):\n",
    "        \"\"\"Given tokenized sequences (either questions, answers or answer\n",
    "        options, tokenized in ``__getitem__``), padding them to maximum\n",
    "        specified sequence length. Return as a tensor of size\n",
    "        ``(*, max_sequence_length)``.\n",
    "\n",
    "        This method is only called in ``__getitem__``, chunked out separately\n",
    "        for readability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequences : List[List[int]]\n",
    "            List of tokenized sequences, each sequence is typically a\n",
    "            List[int].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor, torch.Tensor\n",
    "            Tensor of sequences padded to max length, and length of sequences\n",
    "            before padding.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(len(sequences)):\n",
    "            sequences[i] = sequences[i][\n",
    "                : self.config[\"max_sequence_length\"] - 1\n",
    "            ]\n",
    "        sequence_lengths = [len(sequence) for sequence in sequences]\n",
    "\n",
    "        # Pad all sequences to max_sequence_length.\n",
    "        maxpadded_sequences = torch.full(\n",
    "            (len(sequences), self.config[\"max_sequence_length\"]),\n",
    "            fill_value=self.vocabulary.PAD_INDEX,\n",
    "        )\n",
    "        padded_sequences = pad_sequence(\n",
    "            [torch.tensor(sequence) for sequence in sequences],\n",
    "            batch_first=True,\n",
    "            padding_value=self.vocabulary.PAD_INDEX,\n",
    "        )\n",
    "        maxpadded_sequences[:, : padded_sequences.size(1)] = padded_sequences\n",
    "        return maxpadded_sequences, sequence_lengths\n",
    "\n",
    "    def _get_history(\n",
    "        self,\n",
    "        caption: List[int],\n",
    "        questions: List[List[int]],\n",
    "        answers: List[List[int]],\n",
    "    ):\n",
    "        # Allow double length of caption, equivalent to a concatenated QA pair.\n",
    "        caption = caption[: self.config[\"max_sequence_length\"] * 2 - 1]\n",
    "\n",
    "        for i in range(len(questions)):\n",
    "            questions[i] = questions[i][\n",
    "                : self.config[\"max_sequence_length\"] - 1\n",
    "            ]\n",
    "\n",
    "        for i in range(len(answers)):\n",
    "            answers[i] = answers[i][: self.config[\"max_sequence_length\"] - 1]\n",
    "\n",
    "        # History for first round is caption, else concatenated QA pair of\n",
    "        # previous round.\n",
    "        history = []\n",
    "        history.append(caption)\n",
    "        for question, answer in zip(questions, answers):\n",
    "            history.append(question + answer + [self.vocabulary.EOS_INDEX])\n",
    "        # Drop last entry from history (there's no eleventh question).\n",
    "        history = history[:-1]\n",
    "        max_history_length = self.config[\"max_sequence_length\"] * 2\n",
    "\n",
    "        if self.config.get(\"concat_history\", False):\n",
    "            # Concatenated_history has similar structure as history, except it\n",
    "            # contains concatenated QA pairs from previous rounds.\n",
    "            concatenated_history = []\n",
    "            concatenated_history.append(caption)\n",
    "            for i in range(1, len(history)):\n",
    "                concatenated_history.append([])\n",
    "                for j in range(i + 1):\n",
    "                    concatenated_history[i].extend(history[j])\n",
    "\n",
    "            max_history_length = (\n",
    "                self.config[\"max_sequence_length\"] * 2 * len(history)\n",
    "            )\n",
    "            history = concatenated_history\n",
    "\n",
    "        history_lengths = [len(round_history) for round_history in history]\n",
    "        maxpadded_history = torch.full(\n",
    "            (len(history), max_history_length),\n",
    "            fill_value=self.vocabulary.PAD_INDEX,\n",
    "        )\n",
    "        padded_history = pad_sequence(\n",
    "            [torch.tensor(round_history) for round_history in history],\n",
    "            batch_first=True,\n",
    "            padding_value=self.vocabulary.PAD_INDEX,\n",
    "        )\n",
    "        maxpadded_history[:, : padded_history.size(1)] = padded_history\n",
    "        return maxpadded_history, history_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
