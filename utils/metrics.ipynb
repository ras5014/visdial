{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Metric observes output of certain model, for example, in form of logits or\n",
    "scores, and accumulates a particular metric with reference to some provided\n",
    "targets. In context of VisDial, we use Recall (@ 1, 5, 10), Mean Rank, Mean\n",
    "Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG).\n",
    "\n",
    "Each ``Metric`` must atleast implement three methods:\n",
    "    - ``observe``, update accumulated metric with currently observed outputs\n",
    "      and targets.\n",
    "    - ``retrieve`` to return the accumulated metric., an optionally reset\n",
    "      internally accumulated metric (this is commonly done between two epochs\n",
    "      after validation).\n",
    "    - ``reset`` to explicitly reset the internally accumulated metric.\n",
    "\n",
    "Caveat, if you wish to implement your own class of Metric, make sure you call\n",
    "``detach`` on output tensors (like logits), else it will cause memory leaks.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "\n",
    "def scores_to_ranks(scores: torch.Tensor):\n",
    "    \"\"\"Convert model output scores into ranks.\"\"\"\n",
    "    batch_size, num_rounds, num_options = scores.size()\n",
    "    scores = scores.view(-1, num_options)\n",
    "\n",
    "    # sort in descending order - largest score gets highest rank\n",
    "    sorted_ranks, ranked_idx = scores.sort(1, descending=True)\n",
    "\n",
    "    # i-th position in ranked_idx specifies which score shall take this\n",
    "    # position but we want i-th position to have rank of score at that\n",
    "    # position, do this conversion\n",
    "    ranks = ranked_idx.clone().fill_(0)\n",
    "    for i in range(ranked_idx.size(0)):\n",
    "        for j in range(num_options):\n",
    "            ranks[i][ranked_idx[i][j]] = j\n",
    "    # convert from 0-99 ranks to 1-100 ranks\n",
    "    ranks += 1\n",
    "    ranks = ranks.view(batch_size, num_rounds, num_options)\n",
    "    return ranks\n",
    "\n",
    "\n",
    "class SparseGTMetrics(object):\n",
    "    \"\"\"\n",
    "    A class to accumulate all metrics with sparse ground truth annotations.\n",
    "    These include Recall (@ 1, 5, 10), Mean Rank and Mean Reciprocal Rank.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._rank_list = []\n",
    "\n",
    "    def observe(\n",
    "        self, predicted_scores: torch.Tensor, target_ranks: torch.Tensor\n",
    "    ):\n",
    "        predicted_scores = predicted_scores.detach()\n",
    "\n",
    "        # shape: (batch_size, num_rounds, num_options)\n",
    "        predicted_ranks = scores_to_ranks(predicted_scores)\n",
    "        batch_size, num_rounds, num_options = predicted_ranks.size()\n",
    "\n",
    "        # collapse batch dimension\n",
    "        predicted_ranks = predicted_ranks.view(\n",
    "            batch_size * num_rounds, num_options\n",
    "        )\n",
    "\n",
    "        # shape: (batch_size * num_rounds, )\n",
    "        target_ranks = target_ranks.view(batch_size * num_rounds).long()\n",
    "\n",
    "        # shape: (batch_size * num_rounds, )\n",
    "        predicted_gt_ranks = predicted_ranks[\n",
    "            torch.arange(batch_size * num_rounds), target_ranks\n",
    "        ]\n",
    "        self._rank_list.extend(list(predicted_gt_ranks.cpu().numpy()))\n",
    "\n",
    "    def retrieve(self, reset: bool = True):\n",
    "        num_examples = len(self._rank_list)\n",
    "        if num_examples > 0:\n",
    "            # convert to numpy array for easy calculation.\n",
    "            __rank_list = torch.tensor(self._rank_list).float()\n",
    "            metrics = {\n",
    "                \"r@1\": torch.mean((__rank_list <= 1).float()).item(),\n",
    "                \"r@5\": torch.mean((__rank_list <= 5).float()).item(),\n",
    "                \"r@10\": torch.mean((__rank_list <= 10).float()).item(),\n",
    "                \"mean\": torch.mean(__rank_list).item(),\n",
    "                \"mrr\": torch.mean(__rank_list.reciprocal()).item(),\n",
    "            }\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self._rank_list = []\n",
    "\n",
    "\n",
    "class NDCG(object):\n",
    "    def __init__(self):\n",
    "        self._ndcg_numerator = 0.0\n",
    "        self._ndcg_denominator = 0.0\n",
    "\n",
    "    def observe(\n",
    "        self, predicted_scores: torch.Tensor, target_relevance: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Observe model output scores and target ground truth relevance and\n",
    "        accumulate NDCG metric.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted_scores: torch.Tensor\n",
    "            A tensor of shape (batch_size, num_options), because dense\n",
    "            annotations are available for 1 randomly picked round out of 10.\n",
    "        target_relevance: torch.Tensor\n",
    "            A tensor of shape same as predicted scores, indicating ground truth\n",
    "            relevance of each answer option for a particular round.\n",
    "        \"\"\"\n",
    "        predicted_scores = predicted_scores.detach()\n",
    "\n",
    "        # shape: (batch_size, 1, num_options)\n",
    "        predicted_scores = predicted_scores.unsqueeze(1)\n",
    "        predicted_ranks = scores_to_ranks(predicted_scores)\n",
    "\n",
    "        # shape: (batch_size, num_options)\n",
    "        predicted_ranks = predicted_ranks.squeeze()\n",
    "        batch_size, num_options = predicted_ranks.size()\n",
    "\n",
    "        k = torch.sum(target_relevance != 0, dim=-1)\n",
    "\n",
    "        # shape: (batch_size, num_options)\n",
    "        _, rankings = torch.sort(predicted_ranks, dim=-1)\n",
    "        # Sort relevance in descending order so highest relevance gets top rnk.\n",
    "        _, best_rankings = torch.sort(\n",
    "            target_relevance, dim=-1, descending=True\n",
    "        )\n",
    "\n",
    "        # shape: (batch_size, )\n",
    "        batch_ndcg = []\n",
    "        for batch_index in range(batch_size):\n",
    "            num_relevant = k[batch_index]\n",
    "            dcg = self._dcg(\n",
    "                rankings[batch_index][:num_relevant],\n",
    "                target_relevance[batch_index],\n",
    "            )\n",
    "            best_dcg = self._dcg(\n",
    "                best_rankings[batch_index][:num_relevant],\n",
    "                target_relevance[batch_index],\n",
    "            )\n",
    "            batch_ndcg.append(dcg / best_dcg)\n",
    "\n",
    "        self._ndcg_denominator += batch_size\n",
    "        self._ndcg_numerator += sum(batch_ndcg)\n",
    "\n",
    "    def _dcg(self, rankings: torch.Tensor, relevance: torch.Tensor):\n",
    "        sorted_relevance = relevance[rankings].cpu().float()\n",
    "        discounts = torch.log2(torch.arange(len(rankings)).float() + 2)\n",
    "        return torch.sum(sorted_relevance / discounts, dim=-1)\n",
    "\n",
    "    def retrieve(self, reset: bool = True):\n",
    "        if self._ndcg_denominator > 0:\n",
    "            metrics = {\n",
    "                \"ndcg\": float(self._ndcg_numerator / self._ndcg_denominator)\n",
    "            }\n",
    "        else:\n",
    "            metrics = {}\n",
    "\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self._ndcg_numerator = 0.0\n",
    "        self._ndcg_denominator = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
