{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class DynamicRNN(nn.Module):\n",
    "    def __init__(self, rnn_model):\n",
    "        super().__init__()\n",
    "        self.rnn_model = rnn_model\n",
    "\n",
    "    def forward(self, seq_input, seq_lens, initial_state=None):\n",
    "        \"\"\"A wrapper over pytorch's rnn to handle sequences of variable length.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        seq_input : torch.Tensor\n",
    "            Input sequence tensor (padded) for RNN model.\n",
    "            Shape: (batch_size, max_sequence_length, embed_size)\n",
    "        seq_lens : torch.LongTensor\n",
    "            Length of sequences (b, )\n",
    "        initial_state : torch.Tensor\n",
    "            Initial (hidden, cell) states of RNN model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Single tensor of shape (batch_size, rnn_hidden_size) corresponding\n",
    "            to the outputs of the RNN model at the last time step of each input\n",
    "            sequence.\n",
    "        \"\"\"\n",
    "        max_sequence_length = seq_input.size(1)\n",
    "        sorted_len, fwd_order, bwd_order = self._get_sorted_order(seq_lens)\n",
    "        sorted_seq_input = seq_input.index_select(0, fwd_order)\n",
    "        packed_seq_input = pack_padded_sequence(\n",
    "            sorted_seq_input, lengths=sorted_len, batch_first=True\n",
    "        )\n",
    "\n",
    "        if initial_state is not None:\n",
    "            hx = initial_state\n",
    "            assert hx[0].size(0) == self.rnn_model.num_layers\n",
    "        else:\n",
    "            hx = None\n",
    "\n",
    "        self.rnn_model.flatten_parameters()\n",
    "        outputs, (h_n, c_n) = self.rnn_model(packed_seq_input, hx)\n",
    "\n",
    "        # pick hidden and cell states of last layer\n",
    "        h_n = h_n[-1].index_select(dim=0, index=bwd_order)\n",
    "        c_n = c_n[-1].index_select(dim=0, index=bwd_order)\n",
    "\n",
    "        outputs = pad_packed_sequence(\n",
    "            outputs, batch_first=True, total_length=max_sequence_length\n",
    "        )\n",
    "        return outputs, (h_n, c_n)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sorted_order(lens):\n",
    "        sorted_len, fwd_order = torch.sort(\n",
    "            lens.contiguous().view(-1), 0, descending=True\n",
    "        )\n",
    "        _, bwd_order = torch.sort(fwd_order)\n",
    "        sorted_len = list(sorted_len)\n",
    "        return sorted_len, fwd_order, bwd_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
