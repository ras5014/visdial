{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A checkpoint manager periodically saves model and optimizer as .pth\n",
    "files during training.\n",
    "\n",
    "Checkpoint managers help with experiment reproducibility, they record\n",
    "the commit SHA of your current codebase in the checkpoint saving\n",
    "directory. While loading any checkpoint from other commit, they raise a\n",
    "friendly warning, a signal to inspect commit diffs for potential bugs.\n",
    "Moreover, they copy experiment hyper-parameters as a YAML config in\n",
    "this directory.\n",
    "\n",
    "That said, always run your experiments after committing your changes,\n",
    "this doesn't account for untracked or staged, but uncommitted changes.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from subprocess import PIPE, Popen\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import yaml\n",
    "\n",
    "\n",
    "class CheckpointManager(object):\n",
    "    \"\"\"A checkpoint manager saves state dicts of model and optimizer\n",
    "    as .pth files in a specified directory. This class closely follows\n",
    "    the API of PyTorch optimizers and learning rate schedulers.\n",
    "\n",
    "    Note::\n",
    "        For ``DataParallel`` modules, ``model.module.state_dict()`` is\n",
    "        saved, instead of ``model.state_dict()``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Wrapped model, which needs to be checkpointed.\n",
    "    optimizer: optim.Optimizer\n",
    "        Wrapped optimizer which needs to be checkpointed.\n",
    "    checkpoint_dirpath: str\n",
    "        Path to an empty or non-existent directory to save checkpoints.\n",
    "    step_size: int, optional (default=1)\n",
    "        Period of saving checkpoints.\n",
    "    last_epoch: int, optional (default=-1)\n",
    "        The index of last epoch.\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    >>> model = torch.nn.Linear(10, 2)\n",
    "    >>> optimizer = torch.optim.Adam(model.parameters())\n",
    "    >>> ckpt_manager = CheckpointManager(model, optimizer, \"/tmp/ckpt\")\n",
    "    >>> for epoch in range(20):\n",
    "    ...     for batch in dataloader:\n",
    "    ...         do_iteration(batch)\n",
    "    ...     ckpt_manager.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        checkpoint_dirpath,\n",
    "        step_size=1,\n",
    "        last_epoch=-1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        if not isinstance(model, nn.Module):\n",
    "            raise TypeError(\"{} is not a Module\".format(type(model).__name__))\n",
    "\n",
    "        if not isinstance(optimizer, optim.Optimizer):\n",
    "            raise TypeError(\n",
    "                \"{} is not an Optimizer\".format(type(optimizer).__name__)\n",
    "            )\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.ckpt_dirpath = Path(checkpoint_dirpath)\n",
    "        self.step_size = step_size\n",
    "        self.last_epoch = last_epoch\n",
    "        self.init_directory(**kwargs)\n",
    "\n",
    "    def init_directory(self, config={}):\n",
    "        \"\"\"Initialize empty checkpoint directory and record commit SHA\n",
    "        in it. Also save hyper-parameters config in this directory to\n",
    "        associate checkpoints with their hyper-parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ckpt_dirpath.mkdir(parents=True, exist_ok=True)\n",
    "        # save current git commit hash in this checkpoint directory\n",
    "        commit_sha_subprocess = Popen(\n",
    "            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"], stdout=PIPE, stderr=PIPE\n",
    "        )\n",
    "        commit_sha, _ = commit_sha_subprocess.communicate()\n",
    "        commit_sha = commit_sha.decode(\"utf-8\").strip().replace(\"\\n\", \"\")\n",
    "        commit_sha_filepath = self.ckpt_dirpath / f\".commit-{commit_sha}\"\n",
    "        commit_sha_filepath.touch()\n",
    "        yaml.dump(\n",
    "            config,\n",
    "            open(str(self.ckpt_dirpath / \"config.yml\"), \"w\"),\n",
    "            default_flow_style=False,\n",
    "        )\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        \"\"\"Save checkpoint if step size conditions meet. \"\"\"\n",
    "\n",
    "        if not epoch:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "        if not self.last_epoch % self.step_size:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model\": self._model_state_dict(),\n",
    "                    \"optimizer\": self.optimizer.state_dict(),\n",
    "                },\n",
    "                self.ckpt_dirpath / f\"checkpoint_{self.last_epoch}.pth\",\n",
    "            )\n",
    "\n",
    "    def _model_state_dict(self):\n",
    "        \"\"\"Returns state dict of model, taking care of DataParallel case.\"\"\"\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "            return self.model.module.state_dict()\n",
    "        else:\n",
    "            return self.model.state_dict()\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_pthpath):\n",
    "    \"\"\"Given a path to saved checkpoint, load corresponding state dicts\n",
    "    of model and optimizer from it. This method checks if the current\n",
    "    commit SHA of codebase matches the commit SHA recorded when this\n",
    "    checkpoint was saved by checkpoint manager.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint_pthpath: str or pathlib.Path\n",
    "        Path to saved checkpoint (as created by ``CheckpointManager``).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module, optim.Optimizer\n",
    "        Model and optimizer state dicts loaded from checkpoint.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    UserWarning\n",
    "        If commit SHA do not match, or if the directory doesn't have\n",
    "        the recorded commit SHA.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(checkpoint_pthpath, str):\n",
    "        checkpoint_pthpath = Path(checkpoint_pthpath)\n",
    "    checkpoint_dirpath = checkpoint_pthpath.resolve().parent\n",
    "    checkpoint_commit_sha = list(checkpoint_dirpath.glob(\".commit-*\"))\n",
    "\n",
    "    if len(checkpoint_commit_sha) == 0:\n",
    "        warnings.warn(\n",
    "            \"Commit SHA was not recorded while saving checkpoints.\"\n",
    "        )\n",
    "    else:\n",
    "        # verify commit sha, raise warning if it doesn't match\n",
    "        commit_sha_subprocess = Popen(\n",
    "            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"], stdout=PIPE, stderr=PIPE\n",
    "        )\n",
    "        commit_sha, _ = commit_sha_subprocess.communicate()\n",
    "        commit_sha = commit_sha.decode(\"utf-8\").strip().replace(\"\\n\", \"\")\n",
    "\n",
    "        # remove \".commit-\"\n",
    "        checkpoint_commit_sha = checkpoint_commit_sha[0].name[8:]\n",
    "\n",
    "        if commit_sha != checkpoint_commit_sha:\n",
    "            warnings.warn(\n",
    "                f\"Current commit ({commit_sha}) and the commit \"\n",
    "                f\"({checkpoint_commit_sha}) at which checkpoint was saved,\"\n",
    "                \" are different. This might affect reproducibility.\"\n",
    "            )\n",
    "\n",
    "    # load encoder, decoder, optimizer state_dicts\n",
    "    components = torch.load(checkpoint_pthpath)\n",
    "    return components[\"model\"], components[\"optimizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
